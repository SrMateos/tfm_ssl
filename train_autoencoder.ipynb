{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2ec5ab",
   "metadata": {},
   "source": [
    "# Autoencoder Training - Simplified Version\n",
    "\n",
    "This notebook contains a simplified version of the autoencoder training code for easier debugging and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f9b5c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8aed587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from monai.networks.nets import AutoencoderKL, PatchDiscriminator\n",
    "from monai.utils import set_determinism\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import ConfigParser\n",
    "from constants import TASK1_HN\n",
    "from utils.utils import (\n",
    "    get_data_paths,\n",
    "    get_vae_train_transforms,\n",
    "    get_vae_val_transforms\n",
    ")\n",
    "\n",
    "# Set deterministic behavior\n",
    "set_determinism(42)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144665bd",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edd3066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Patch size: [64, 64, 64]\n",
      "  Batch size: 1\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.0001\n",
      "  Debug mode: False\n"
     ]
    }
   ],
   "source": [
    "patch_size = [64, 64, 64]\n",
    "train_val_split = 0.8\n",
    "task1 = True\n",
    "\n",
    "debug_mode = False\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "adv_weight = 0.01\n",
    "perceptual_weight = 0.001\n",
    "kl_weight = 1e-6\n",
    "autoencoder_warm_up_n_epochs = 3\n",
    "val_interval = 2\n",
    "save_interval = 5\n",
    "\n",
    "sw_batch_size = 1\n",
    "overlap = 0.5\n",
    "mode = \"gaussian\"\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Patch size: {patch_size}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Debug mode: {debug_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117fd35",
   "metadata": {},
   "source": [
    "## 3. KL Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2c0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mu, z_sigma):\n",
    "    \"\"\"\n",
    "    KL divergence loss for VAE\n",
    "    kl_loss = 0.5 * sum(z_mu^2 + z_sigma^2 - log(z_sigma^2) - 1)\n",
    "    \"\"\"\n",
    "    klloss = 0.5 * torch.sum(\n",
    "        z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1,\n",
    "        dim=list(range(1, len(z_mu.shape)))\n",
    "    )\n",
    "    return torch.sum(klloss) / klloss.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c47978",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51788640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from [WindowsPath('data/SynthRAD2025/synthRAD2025_Task1_Train_D/synthRAD2025_Task1_Train_D/Task1/HN'), WindowsPath('data/synthRAD2025_Task1_Train/Task1/HN')]...\n",
      "Total samples: 221\n",
      "First data sample: {'image': 'data\\\\SynthRAD2025\\\\synthRAD2025_Task1_Train_D\\\\synthRAD2025_Task1_Train_D\\\\Task1\\\\HN\\\\1HND001\\\\ct.mha', 'mask': 'data\\\\SynthRAD2025\\\\synthRAD2025_Task1_Train_D\\\\synthRAD2025_Task1_Train_D\\\\Task1\\\\HN\\\\1HND001\\\\mask.mha'}\n",
      "Training samples: 176\n",
      "Validation samples: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 176/176 [00:57<00:00,  3.04it/s]\n",
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "Loading dataset: 100%|██████████| 45/45 [00:08<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATASET = \"task1_hn\"\n",
    "print(f\"Loading data from {TASK1_HN}...\")\n",
    "_, cts_paths, masks_paths = get_data_paths(TASK1_HN, task1=task1, debug=debug_mode)\n",
    "data = [{\"image\": ct, \"mask\": mask} for ct, mask in zip(cts_paths, masks_paths)]\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"First data sample: {data[0]}\")\n",
    "\n",
    "# Split data\n",
    "train_data_split = int(len(data) * train_val_split)\n",
    "train_data = data[:train_data_split]\n",
    "val_data = data[train_data_split:]\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = CacheDataset(data=train_data, transform=get_vae_train_transforms())\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True) #, collate_fn=pad_list_data_collate)\n",
    "\n",
    "val_ds = CacheDataset(data=val_data, transform=get_vae_val_transforms())\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False) #, collate_fn=pad_list_data_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d75e0",
   "metadata": {},
   "source": [
    "## 4.1 GENERAR JSON CON DATOS DEL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata from datasets...\n",
      "Extracted metadata from 176 training samples and 45 validation samples\n",
      "Calculated metadata statistics\n",
      "Dataset metadata saved to: notebook_outputs\\dataset_metadata.json\n",
      "Total training samples: 176\n",
      "Total validation samples: 45\n",
      "\n",
      "Sample metadata structure:\n",
      "Mean spacing: [1.0, 1.0, 3.0]\n",
      "Mean spatial shape: [340.47058823529414, 308.4117647058824, 90.13122171945702]\n",
      "Space: RAS\n",
      "\n",
      "First training sample (Linux format):\n",
      "Image: data/SynthRAD2025/synthRAD2025_Task1_Train_D/synthRAD2025_Task1_Train_D/Task1/HN/1HND001/ct.mha\n",
      "Mask: data/SynthRAD2025/synthRAD2025_Task1_Train_D/synthRAD2025_Task1_Train_D/Task1/HN/1HND001/mask.mha\n",
      "\n",
      "First validation sample (Linux format):\n",
      "Image: data/synthRAD2025_Task1_Train/Task1/HN/1HNC037/ct.mha\n",
      "Mask: data/synthRAD2025_Task1_Train/Task1/HN/1HNC037/mask.mha\n",
      "Extracted metadata from 176 training samples and 45 validation samples\n",
      "Calculated metadata statistics\n",
      "Dataset metadata saved to: notebook_outputs\\dataset_metadata.json\n",
      "Total training samples: 176\n",
      "Total validation samples: 45\n",
      "\n",
      "Sample metadata structure:\n",
      "Mean spacing: [1.0, 1.0, 3.0]\n",
      "Mean spatial shape: [340.47058823529414, 308.4117647058824, 90.13122171945702]\n",
      "Space: RAS\n",
      "\n",
      "First training sample (Linux format):\n",
      "Image: data/SynthRAD2025/synthRAD2025_Task1_Train_D/synthRAD2025_Task1_Train_D/Task1/HN/1HND001/ct.mha\n",
      "Mask: data/SynthRAD2025/synthRAD2025_Task1_Train_D/synthRAD2025_Task1_Train_D/Task1/HN/1HND001/mask.mha\n",
      "\n",
      "First validation sample (Linux format):\n",
      "Image: data/synthRAD2025_Task1_Train/Task1/HN/1HNC037/ct.mha\n",
      "Mask: data/synthRAD2025_Task1_Train/Task1/HN/1HNC037/mask.mha\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Function to convert Windows path to Linux format\n",
    "def convert_to_linux_path(windows_path):\n",
    "    \"\"\"Convert Windows path to Linux-compatible format\"\"\"\n",
    "    if isinstance(windows_path, str):\n",
    "        # Convert backslashes to forward slashes\n",
    "        linux_path = windows_path.replace('\\\\', '/')\n",
    "        # Remove drive letter if present (C:/ -> /)\n",
    "        if len(linux_path) > 1 and linux_path[1] == ':':\n",
    "            linux_path = linux_path[2:]  # Remove \"C:\"\n",
    "        return linux_path\n",
    "    return windows_path\n",
    "\n",
    "# Function to extract metadata from a dataset\n",
    "def extract_metadata_from_dataset(dataset):\n",
    "    \"\"\"Extract metadata from all samples in a dataset\"\"\"\n",
    "    metadata_list = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        # Get the metadata from the sample\n",
    "        img_meta = sample['image_meta_dict'] if 'image_meta_dict' in sample else sample.get('image').meta if hasattr(sample.get('image'), 'meta') else {}\n",
    "        mask_meta = sample['mask_meta_dict'] if 'mask_meta_dict' in sample else sample.get('mask').meta if hasattr(sample.get('mask'), 'meta') else {}\n",
    "\n",
    "        # Combine metadata (prioritize image metadata)\n",
    "        combined_meta = {**mask_meta, **img_meta}\n",
    "        metadata_list.append(combined_meta)\n",
    "\n",
    "    return metadata_list\n",
    "\n",
    "# Function to calculate mean and std for numerical fields\n",
    "def calculate_stats(metadata_list):\n",
    "    \"\"\"Calculate mean and std for numerical metadata fields\"\"\"\n",
    "    # Fields to calculate statistics for\n",
    "    numerical_fields = ['spacing', 'spatial_shape']\n",
    "\n",
    "    stats = {'mean': {}, 'std': {}}\n",
    "\n",
    "    # Collect all values for each field\n",
    "    field_values = defaultdict(list)\n",
    "\n",
    "    for meta in metadata_list:\n",
    "        for field in numerical_fields:\n",
    "            if field in meta:\n",
    "                if field == 'spacing' and hasattr(meta[field], 'tolist'):\n",
    "                    field_values[field].append(meta[field].tolist())\n",
    "                elif field == 'spatial_shape' and hasattr(meta[field], 'tolist'):\n",
    "                    field_values[field].append(meta[field].tolist())\n",
    "                elif isinstance(meta[field], (list, tuple, np.ndarray)):\n",
    "                    field_values[field].append(list(meta[field]))\n",
    "                else:\n",
    "                    field_values[field].append(meta[field])\n",
    "\n",
    "    # Calculate mean and std\n",
    "    for field, values in field_values.items():\n",
    "        if values:\n",
    "            if field in ['spacing', 'spatial_shape']:\n",
    "                # For arrays, calculate element-wise mean and std\n",
    "                values_array = np.array(values)\n",
    "                stats['mean'][field] = np.mean(values_array, axis=0).tolist()\n",
    "                stats['std'][field] = np.std(values_array, axis=0).tolist()\n",
    "\n",
    "    # Handle other metadata fields (take first available or create defaults)\n",
    "    sample_meta = metadata_list[0] if metadata_list else {}\n",
    "\n",
    "    # Add other fields with default values or from first sample\n",
    "    for field in ['original_affine', 'affine']:\n",
    "        if field in sample_meta:\n",
    "            if hasattr(sample_meta[field], 'tolist'):\n",
    "                stats['mean'][field] = sample_meta[field].tolist()\n",
    "                stats['std'][field] = np.zeros_like(sample_meta[field]).tolist()\n",
    "            else:\n",
    "                stats['mean'][field] = sample_meta[field]\n",
    "                stats['std'][field] = 0\n",
    "        else:\n",
    "            # Default 4x4 identity matrix for affine transforms\n",
    "            default_affine = np.eye(4).tolist()\n",
    "            stats['mean'][field] = default_affine\n",
    "            stats['std'][field] = np.zeros((4, 4)).tolist()\n",
    "\n",
    "    # Add string fields\n",
    "    for field in ['space', 'original_channel_dim']:\n",
    "        if field in sample_meta:\n",
    "            stats['mean'][field] = sample_meta[field]\n",
    "            stats['std'][field] = 0  # No std for categorical data\n",
    "        else:\n",
    "            stats['mean'][field] = 'unknown'\n",
    "            stats['std'][field] = 0\n",
    "\n",
    "    return stats\n",
    "\n",
    "print(\"Extracting metadata from datasets...\")\n",
    "\n",
    "# Extract metadata from train and validation datasets\n",
    "train_metadata = extract_metadata_from_dataset(train_ds)\n",
    "val_metadata = extract_metadata_from_dataset(val_ds)\n",
    "\n",
    "# Combine all metadata for statistics calculation\n",
    "all_metadata = train_metadata + val_metadata\n",
    "\n",
    "print(f\"Extracted metadata from {len(train_metadata)} training samples and {len(val_metadata)} validation samples\")\n",
    "\n",
    "# Calculate statistics\n",
    "stats = calculate_stats(all_metadata)\n",
    "\n",
    "print(\"Calculated metadata statistics\")\n",
    "\n",
    "# Create the dataset dump structure\n",
    "dataset_dump = {\n",
    "    \"metadata\": {\n",
    "        \"name\": \"SynthRAD2025_Task1_HN\",\n",
    "        \"mean\": stats['mean'],\n",
    "        \"std\": stats['std']\n",
    "    },\n",
    "    \"train\": [],\n",
    "    \"validation\": []\n",
    "}\n",
    "\n",
    "# Add training data paths (converted to Linux format)\n",
    "for data_item in train_data:\n",
    "    dataset_dump[\"train\"].append({\n",
    "        \"image\": convert_to_linux_path(data_item[\"image\"]),\n",
    "        \"mask\": convert_to_linux_path(data_item[\"mask\"])\n",
    "    })\n",
    "\n",
    "# Add validation data paths (converted to Linux format)\n",
    "for data_item in val_data:\n",
    "    dataset_dump[\"validation\"].append({\n",
    "        \"image\": convert_to_linux_path(data_item[\"image\"]),\n",
    "        \"mask\": convert_to_linux_path(data_item[\"mask\"])\n",
    "    })\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"notebook_outputs\", exist_ok=True)\n",
    "\n",
    "# Save the dataset dump\n",
    "output_file = os.path.join(\"notebook_outputs\", f\"dataset_{DATASET}.json\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_dump, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Dataset metadata saved to: {output_file}\")\n",
    "print(f\"Total training samples: {len(dataset_dump['train'])}\")\n",
    "print(f\"Total validation samples: {len(dataset_dump['validation'])}\")\n",
    "\n",
    "# Display sample of the metadata\n",
    "print(\"\\nSample metadata structure:\")\n",
    "print(f\"Mean spacing: {stats['mean'].get('spacing', 'N/A')}\")\n",
    "print(f\"Mean spatial shape: {stats['mean'].get('spatial_shape', 'N/A')}\")\n",
    "print(f\"Space: {stats['mean'].get('space', 'N/A')}\")\n",
    "print(f\"\\nFirst training sample (Linux format):\")\n",
    "if dataset_dump['train']:\n",
    "    print(f\"Image: {dataset_dump['train'][0]['image']}\")\n",
    "    print(f\"Mask: {dataset_dump['train'][0]['mask']}\")\n",
    "print(f\"\\nFirst validation sample (Linux format):\")\n",
    "if dataset_dump['validation']:\n",
    "    print(f\"Image: {dataset_dump['validation'][0]['image']}\")\n",
    "    print(f\"Mask: {dataset_dump['validation'][0]['mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33148d85",
   "metadata": {},
   "source": [
    "## 5. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Autoencoder parameters: 2,299,208\n",
      "Discriminator parameters: 2,770,977\n",
      "Total size: 5.07 MB\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = AutoencoderKL(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(32, 64, 64),\n",
    "    latent_channels=3,\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ").to(device)\n",
    "\n",
    "# Discriminator\n",
    "discriminator = PatchDiscriminator(\n",
    "    spatial_dims=3,\n",
    "    num_layers_d=3,\n",
    "    channels=32,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    norm=\"INSTANCE\",\n",
    ").to(device)\n",
    "\n",
    "print(f\"Autoencoder parameters: {sum(p.numel() for p in autoencoder.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(f\"Total models size: {(sum(p.numel() for p in autoencoder.parameters()) + sum(p.numel() for p in discriminator.parameters())) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd85ca",
   "metadata": {},
   "source": [
    "## 6. Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165afef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions and optimizers initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "# Loss functions\n",
    "l1_loss = L1Loss()\n",
    "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "loss_perceptual = PerceptualLoss(\n",
    "    spatial_dims=3,\n",
    "    network_type=\"squeeze\",\n",
    "    is_fake_3d=True,\n",
    "    fake_3d_ratio=0.2\n",
    ").to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_g = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate)\n",
    "optimizer_d = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Loss functions and optimizers initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e3b49",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ecc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"notebook_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    autoencoder.train()\n",
    "    discriminator.train()\n",
    "    epoch_start = time.time()\n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    epoch_kl_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch_data in progress_bar:\n",
    "        step += 1\n",
    "        images = batch_data[\"image\"].to(device)\n",
    "\n",
    "        # Train Generator (Autoencoder)\n",
    "        optimizer_g.zero_grad(set_to_none=True)\n",
    "\n",
    "        reconstruction, z_mu, z_sigma = autoencoder(images)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss_recon = l1_loss(reconstruction, images)\n",
    "        loss_kl = kl_loss(z_mu, z_sigma)\n",
    "        loss_perc = loss_perceptual(reconstruction.float(), images.float())\n",
    "\n",
    "        # Base generator loss\n",
    "        loss_g_base = loss_recon + kl_weight * loss_kl + perceptual_weight * loss_perc\n",
    "\n",
    "        # Add adversarial loss after warmup\n",
    "        if epoch >= autoencoder_warm_up_n_epochs:\n",
    "            logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
    "            loss_g_adv = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "            loss_g = loss_g_base + adv_weight * loss_g_adv\n",
    "        else:\n",
    "            loss_g = loss_g_base\n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Train Discriminator (after warmup)\n",
    "        loss_d = torch.tensor(0.0)\n",
    "        if epoch >= autoencoder_warm_up_n_epochs:\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Fake samples\n",
    "            logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "            loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "\n",
    "            # Real samples\n",
    "            logits_real = discriminator(images.contiguous().detach())[-1]\n",
    "            loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "\n",
    "            loss_d = adv_weight * (loss_d_fake + loss_d_real) * 0.5\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        epoch_g_loss += loss_g.item()\n",
    "        epoch_d_loss += loss_d.item()\n",
    "        epoch_recon_loss += loss_recon.item()\n",
    "        epoch_kl_loss += loss_kl.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            \"G_loss\": f\"{loss_g.item():.4f}\",\n",
    "            \"D_loss\": f\"{loss_d.item():.4f}\",\n",
    "            \"Recon\": f\"{loss_recon.item():.4f}\"\n",
    "        })\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_g_loss = epoch_g_loss / step\n",
    "    avg_d_loss = epoch_d_loss / step\n",
    "    avg_recon_loss = epoch_recon_loss / step\n",
    "    avg_kl_loss = epoch_kl_loss / step\n",
    "\n",
    "    train_losses.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"g_loss\": avg_g_loss,\n",
    "        \"d_loss\": avg_d_loss,\n",
    "        \"recon_loss\": avg_recon_loss,\n",
    "        \"kl_loss\": avg_kl_loss\n",
    "    })\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s\")\n",
    "    print(f\"  G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}\")\n",
    "    print(f\"  Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        autoencoder.eval()\n",
    "        val_recon_loss = 0\n",
    "        val_step = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                val_step += 1\n",
    "                val_images = val_batch[\"image\"].to(device)\n",
    "\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    inputs=val_images,\n",
    "                    roi_size=patch_size,\n",
    "                    sw_batch_size=sw_batch_size,\n",
    "                    predictor=autoencoder.reconstruct,\n",
    "                    overlap=overlap,\n",
    "                    mode=mode,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                batch_recon_loss = l1_loss(val_outputs, val_images)\n",
    "                val_recon_loss += batch_recon_loss.item()\n",
    "\n",
    "        avg_val_loss = val_recon_loss / val_step\n",
    "        val_losses.append({\"epoch\": epoch, \"val_loss\": avg_val_loss})\n",
    "\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(autoencoder.state_dict(),\n",
    "                      os.path.join(output_dir, \"best_autoencoder.pth\"))\n",
    "            torch.save(discriminator.state_dict(),\n",
    "                      os.path.join(output_dir, \"best_discriminator.pth\"))\n",
    "            print(f\"  New best model saved! Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"autoencoder_state_dict\": autoencoder.state_dict(),\n",
    "            \"discriminator_state_dict\": discriminator.state_dict(),\n",
    "            \"optimizer_g_state_dict\": optimizer_g.state_dict(),\n",
    "            \"optimizer_d_state_dict\": optimizer_d.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "        print(f\"  Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356c84c",
   "metadata": {},
   "source": [
    "## 8. Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d25da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs = [loss[\"epoch\"] for loss in train_losses]\n",
    "g_losses = [loss[\"g_loss\"] for loss in train_losses]\n",
    "d_losses = [loss[\"d_loss\"] for loss in train_losses]\n",
    "recon_losses = [loss[\"recon_loss\"] for loss in train_losses]\n",
    "kl_losses = [loss[\"kl_loss\"] for loss in train_losses]\n",
    "\n",
    "# Generator loss\n",
    "axes[0, 0].plot(epochs, g_losses, 'b-', label='Generator Loss')\n",
    "axes[0, 0].set_title('Generator Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Discriminator loss\n",
    "axes[0, 1].plot(epochs, d_losses, 'r-', label='Discriminator Loss')\n",
    "axes[0, 1].set_title('Discriminator Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Reconstruction loss\n",
    "axes[1, 0].plot(epochs, recon_losses, 'g-', label='Reconstruction Loss')\n",
    "axes[1, 0].set_title('Reconstruction Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# KL loss\n",
    "axes[1, 1].plot(epochs, kl_losses, 'm-', label='KL Loss')\n",
    "axes[1, 1].set_title('KL Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'training_losses.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot validation loss if available\n",
    "if val_losses:\n",
    "    val_epochs = [loss[\"epoch\"] for loss in val_losses]\n",
    "    val_loss_values = [loss[\"val_loss\"] for loss in val_losses]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(val_epochs, val_loss_values, 'o-', label='Validation Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'validation_loss.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b12515",
   "metadata": {},
   "source": [
    "## 9. Save Final Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0330fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final models\n",
    "torch.save(autoencoder.state_dict(), os.path.join(output_dir, \"final_autoencoder.pth\"))\n",
    "torch.save(discriminator.state_dict(), os.path.join(output_dir, \"final_discriminator.pth\"))\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "\n",
    "training_history = {\n",
    "    \"config\": {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"patch_size\": patch_size,\n",
    "        \"warmup_epochs\": autoencoder_warm_up_n_epochs\n",
    "    },\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"best_val_loss\": best_val_loss\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"training_history.json\"), \"w\") as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"All outputs saved to: {output_dir}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee91b6",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "autoencoder.load_state_dict(torch.load(os.path.join(output_dir, \"best_autoencoder.pth\")))\n",
    "autoencoder.eval()\n",
    "\n",
    "# Generate sample reconstruction\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(val_loader))\n",
    "    sample_image = sample_batch[\"image\"][:1].to(device)  # Take first sample\n",
    "\n",
    "    reconstruction = sliding_window_inference(\n",
    "        inputs=sample_image,\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=sw_batch_size,\n",
    "        predictor=autoencoder.reconstruct,\n",
    "        overlap=overlap,\n",
    "        mode=mode,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Show middle slice\n",
    "    middle_slice = sample_image.shape[-1] // 2\n",
    "\n",
    "    original_slice = sample_image[0, 0, :, :, middle_slice].cpu().numpy()\n",
    "    recon_slice = reconstruction[0, 0, :, :, middle_slice].cpu().numpy()\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    ax1.imshow(original_slice, cmap='gray')\n",
    "    ax1.set_title('Original')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(recon_slice, cmap='gray')\n",
    "    ax2.set_title('Reconstruction')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Difference\n",
    "    diff = abs(original_slice - recon_slice)\n",
    "    ax3.imshow(diff, cmap='hot')\n",
    "    ax3.set_title('Difference')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'sample_reconstruction.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate reconstruction error\n",
    "    mse = torch.mean((sample_image - reconstruction) ** 2).item()\n",
    "    mae = torch.mean(torch.abs(sample_image - reconstruction)).item()\n",
    "\n",
    "    print(f\"Sample reconstruction metrics:\")\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
